% Some commands used in this file
\newcommand{\package}{\emph}

\chapter{Introduction}

Validating software applications and systems for correct behavior can be done in many ways. Automated or manual testing, formal verification, and model checking are well-known techniques that ensure the correctness of software and are usually part of the development process. Runtime verification is a lightweight formal verification technique mainly concerned with checking the behavior of a system during its runtime. Runtime verification generally validate that a given program trace adheres to predefined properties.

In the heart of runtime verification lies the \textit{monitor}. It accepts one or many properties formulated in a specification language, a program trace to monitor, and returns a verdict indicating if the given trace satisfies the formulated properties. The process of gathering a program trace may differ based on the architecture of an application: if runtime verification is part of the monitored system itself, a stream of internal events can be monitored directly. In other cases, application logs may approximate the actual program trace.

Similarly, there is a variety of specification languages for defining the set of desired and undesired program traces. While we focus on temporal logic-based languages, others may be based on regular expressions, streams, or a combination of these families \cite{bartocciIntroductionRuntimeVerification2018}. Early iterations of temporal logic-based specification languages, such as CARET \cite{alurTemporalLogicNested2004}, regard events of a program trace as atomic symbols, for instance, $\mathsf{openFile}$ or $\mathsf{closeFile}$. Later, languages were established that support the formalization of properties on streams of events that carry additional data \cite{havelundMonitoringEventsThat2018}.

Depending on the syntax and semantics of the specification language and the capabilities of the underlying monitor, the domain of event data may be limited. While some runtime verification tools such as HLola \cite{gorostiagaHLolaVeryFunctional2021}, \textsc{ParTraP} \cite{bleinExtendingSpecificationPatterns2018}, and \textsc{LogScope} \cite{barringerFormalAnalysisLog2010} provide support for custom event data types, many available tools limit the domain to primitive types, namely boolean, numerical, and string types. In contrast, JSON formatted logs have become ubiquitous, allowing applications to output events as arbitrarily complex data structures. To monitor complex typed event streams using a runtime verification tool with limited type support, the event stream must first be transformed into a supported format. We distinguish two possible approaches: First, a general preprocessor transforms the event stream, independent of the application-specific semantics of the data structures carried by events. The preprocessor can thus be reused, but the transformation increases the complexity of the expressed properties in the given specification language. Second, an application- and property-specific preprocessor extracts only relevant event data into a supported format. In this case, the formalization of properties in the specification language is generally less complex, but the preprocessor must be potentially adjusted or even rewritten whenever the desired properties or application log formats change. Both approaches increase the risk of introducing subtle bugs into the monitoring process.

To improve the runtime verification of complex typed event streams, we extend the existing monitor system \MonPoly \cite{basinMonPolyMonitoringTool2017} and its specification language metric first-order dynamic logic (MFODL) \cite{basinFormallyVerifiedOptimized2020}. The goal is to support monitoring MFODL properties over JSON-formatted application logs as an event source, without or only minimal preprocessing. At the same time, specifications should be expressible naturally, hence improving the development experience and readability of formulas. We achieve this by the following contributions:
\begin{compactitem}
	\item Language extension: We introduce complex-typed MFODL (CMFODL) by extending the syntax and semantics of MFODL. CMFODL supports variables and constants of custom product types. Projections on variables can be used to access values of nested fields.
	\item CMFODL compiler: We implement a formula compiler, translating CMFODL properties to semantically equivalent MFODL formulas, which can be consumed and monitored by \MonPolyN.
	\item JSON log parser: A dedicated log parser for JSON logs that acts as a generalized event stream preprocessor: The stream of complex-typed event data is transformed into a stream of finite relations, as expected by \MonPoly and the compiler.
\end{compactitem}
Compared to existing temporal specification languages with support for custom typed event data, CMFODL inherits MFODL features such as global quantification, referencing past event data and support for real-time monitoring, providing a unique combination of features.

Writing formulas is hard, especially those that belong to a monitorable fragment supported by \MonPolyN. We improve the usability of CMFODL with the following contributions:
\begin{compactitem}
	\item A static type-checking algorithm with type inference support, helping users in writing correct formulas while keeping the additional effort minimal.
	\item An extended monitorability check that helps users to better understand why a given CMFODL formula is not monitorable by \MonPolyN.
\end{compactitem}

The structure of this thesis is as follows: Chapters \ref{chap:related_work} and \ref{chap:background} present related work and give an overview of the background to this work. Chapters \ref{chap:complex_data_types} and \ref{chap:type_checking} describe the language extension CMFODL and formalize its type system. Chapters \ref{chap:compilation} and \ref{chap:monitorability} introduce the preprocessing of event streams and compilation of CMFODL formulas. Chapter \ref{chap:extending_monpoly} gives an overview of the changes introduced to the monitoring tool \MonPolyN. Finally, Chapter \ref{chap:case_study} presents a case study on existing approaches of generalized and specialized preprocessing, and demonstrates the added value of this work in real-world scenarios.

